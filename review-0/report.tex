\documentclass[12pt,a4paper]{article}
\usepackage{ssnreview}
\usepackage{float}
\usepackage{url}
\usepackage{alltt}
\usepackage{longtable}
%\usepackage{mathtools}
\usepackage{algorithm2e}[1]
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{trees}

\begin{document}

\ptitle{Sample-Efficiency in Complex Environments using Reinforcement Learning}
\setnauthors{3}
\setauthorone{Nimish S}{312217104098}
\setauthortwo{*****}{312217104***}
\setauthorthree{*****}{312217104***}
\semester{7}
\guide{*****}
\review{0}
\reviewdate{24 October 2020}
\reviewtitle
\hrule

\section{Abstract}
Current Reinforcement Learning algorithms excel at performing in
non-hierarchial environments. However, when it comes to complex,
hierarchial \& sparse environments  they are impractical to use 
as sample-inefficiency becomes more prominent due to the curse of
dimensionality. This makes reinforcement learning impractical to 
use in many real-world situations as many  of these environments are 
complex, hierarchial \& sparse in  nature. Superior exploration techniques 
like Curiosity-driven Exploration have proven to improve sample efficiency 
in simple environments. We propose to use such techniques in combination 
with RL algorithms like DDPG and DQN to succesfully navigate a complex 
environment.

\section{Introduction}
Reinforcement learning algorithms aim at learning policies for achieving 
target tasks by maximizing rewards provided by the environment. Reinforcement
learning combined with neural networks has recently led to a wide range of 
successes in learning policies for sequential decision-making problems. From a
practical standpoint, RL is currently used in simple environments where PID 
\textit{(Proportional Integral Derivative)} controllers cannot be easily applied
(e.g. simple robotic control, HVACs \& autonomous vehicles). Currently, 
it is crucial for the environments to be designed in such a way as to encourage 
learning.

However, in most real-world applications, there are three important environmental
characteristics most reinforcement learning algorithms struggle with. The 
rewards supplied to the agent are extremely sparse or sometimes missing 
altogether. The actions  to be taken are hierarchial in nature (i.e. 
each goal state does not result in termination, rather opens the possibility 
to new goal states). The state space is extremely high dimensional leading 
to a lot of time spent on extracting useful features from higher-level 
representations. A combination of all these problems results in extremely 
sample-inefficient performance of existing algorithms.

To put things into perspective, we compare the current methods published 
publicly by the team at \textit{OpenAI}. In order to attain human-level control
in an extremely complex environment, a \textit{Proximal Policy Optimization}
agent was  trained over a period of 10 months. It was trained using
128,000 CPU cores and 256 GPUs and read in 1,048,576 observations 
every second. It used an unimaginably large number of samples, 
simply impractical for use on a large scale.

% our solution

\section{Literature survey}
Some of the existing approaches use Inductive Logic Programming that
do not use probabilistic graphical model to compute conditional
probabilities for inferred facts. To avoid this, a statistical
relational learning approach is adopted. SRL handles both uncertainty
and structured data, integrating first-order logic and probabilistic
graphical models. The alternative is to take SRL approaches such as
Markov Logic Networks (MLN) framework for both learning first order
rules and probabilistic inference of additional facts. But MLN may
result in an intractably large graphical model for large datasets.

\section{Proposed system}

A Rule Learner is applied on the set of facts an IE system has
extracted from the document. The Rule Learner first updates the
frequency of occurrence of each relational predicate. Then it builds a
Bayesian network whose nodes represent relation extractions. It then
traverses the graph to know the first order rules. The learner
traverses the resulting graph to construct rules. For each directed
edge $(x,y)$ in the graph, it constructs a rule in which the body
contains $x$ and the head is $y$ head. System architecture for
inferring implicit facts using BLPs is shown in Figure~\ref{arch}.


\begin{thebibliography}{99}
  \bibliographystyle{plain}
  
\bibitem[1]{sindhu2013} Sindhu Raghavan, Raymond J. Mooney, {\em
    Online Inference-Rule Learning from Natural-Language Extractions},
  University of Texas at Austin, In Proceedings of the 3rd Statistical
  Relational AI (StaRAI-13) workshop at AAAI '13, July 2013.

\bibitem[2]{sindhu2012}Sindhu Raghavan, Raymond J. Mooney, and
  Ku. H,{\em to read between the lines using Bayesian Logic Programs},
  In Proceedings of ACL 2012.

\bibitem[3]{cowir1996}Cowie, J., and Lehnert, W. ,{\em Information
    extraction}, 1996 CACM.

\bibitem[4]{kersting2007}Kersting, K., and De Raedt, L., {\em Bayesian
    Logic Programming: Theory and tool}, 2007

\bibitem[5]{5} In Getoor, L., and Taskar, B, {\em Introduction to
    Statistical Relational Learning}, Cambridge, MA: MIT Press.
\end{thebibliography}


	
\end{document}
